<div align="center">

# AI-Safety SCAV

<div>
    <a href="https://arxiv.org/abs/2404.12038" target="_blank">
        <img alt="paper" src="https://img.shields.io/badge/cs.CL-	arXiv:2404.12038-red">
    </a>
    <a href="mailto:xitingwang@ruc.edu.cn" target="_blank">
        <img alt="email" src="https://img.shields.io/badge/ðŸ“® enquiry-blue">
    </a>
</div>

</div>

This is the code for our NeurIPS 2024 paper *<strong>Uncovering Safety Risks of Large Language Models through Concept Activation Vector</strong>*.

## News

- [2024-10-28] The code for prompt-level attack is released.
- [2024-09-30] The code for embedding-level attack is released.
- [2024-04-18] The paper is available on arXiv.

## Citation

If you find this work helpful, please consider citing our paper:

```bibtex
@inproceedings{Xu2024uncovering,
  title  = {Uncovering Safety Risks of Large Language Models through Concept Activation Vector},
  author = {Zhihao Xu and Ruixuan Huang and Changyu Chen and Xiting Wang},
  year   = {2024},
  url    = {https://openreview.net/forum?id=Uymv9ThB50}
}
```

## Disclaimer

This project may lead to attacks on LLMs and is intended for academic research use only. It is prohibited for illegal purposes. The authors have shared the vulnerabilities with OpenAI and Microsoft.
